{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/8a/6a/19e9fe04fca059ccf770861c7d5721ab4c2aebc539889e97c7977528a53b/pip-24.0-py3-none-any.whl.metadata\n",
      "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m0m\n",
      "\u001B[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-24.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.51.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (159 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m159.5/159.5 kB\u001B[0m \u001B[31m872.7 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from matplotlib) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.4-cp39-cp39-macosx_11_0_arm64.whl (7.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.5/7.5 MB\u001B[0m \u001B[31m13.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading contourpy-1.2.1-cp39-cp39-macosx_11_0_arm64.whl (244 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m245.0/245.0 kB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.51.0-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.8/2.8 MB\u001B[0m \u001B[31m7.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\n",
      "\u001B[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading kiwisolver-1.4.5-cp39-cp39-macosx_11_0_arm64.whl (66 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.2/66.2 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pillow-10.3.0-cp39-cp39-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m30.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m103.2/103.2 kB\u001B[0m \u001B[31m11.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 importlib-resources-6.4.0 kiwisolver-1.4.5 matplotlib-3.8.4 pillow-10.3.0 pyparsing-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (1.13.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp39-cp39-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.4.1.post1-cp39-cp39-macosx_12_0_arm64.whl (10.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/10.5 MB\u001B[0m \u001B[31m18.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m301.2/301.2 kB\u001B[0m \u001B[31m15.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.0 scikit-learn-1.4.1.post1 threadpoolctl-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (0.16.6)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (1.44.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (68.2.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dgl in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from dgl) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from dgl) (1.13.0)\n",
      "Requirement already satisfied: networkx>=2.1 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from dgl) (3.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from dgl) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from dgl) (4.66.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from dgl) (5.9.8)\n",
      "Requirement already satisfied: torchdata>=0.5.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from dgl) (0.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from requests>=2.19.0->dgl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from requests>=2.19.0->dgl) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
      "Requirement already satisfied: torch>=2 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torchdata>=0.5.0->dgl) (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2.1\n",
      "  Downloading torch-2.2.1-cp39-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch==2.2.1) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch==2.2.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch==2.2.1) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch==2.2.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch==2.2.1) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from torch==2.2.1) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from jinja2->torch==2.2.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from sympy->torch==2.2.1) (1.3.0)\n",
      "Downloading torch-2.2.1-cp39-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.7/59.7 MB\u001B[0m \u001B[31m9.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\n",
      "\u001B[?25hInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "Successfully installed torch-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (2.6.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from pydantic) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages (from pydantic) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /Users/andrrosya/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import dgl\n",
    "# c5ad07b19c6c040d7fe9f4fa70aff9783a12e4ce\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "file_path = os.path.join(os.path.dirname(current_directory), 'Dynamic data.xlsx')\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEARS</th>\n",
       "      <th>WBHP:I1</th>\n",
       "      <th>WBHP:I2</th>\n",
       "      <th>WBHP:I3</th>\n",
       "      <th>WBHP:P1</th>\n",
       "      <th>WBHP:P2</th>\n",
       "      <th>WBHP:P3</th>\n",
       "      <th>WLPR:P1</th>\n",
       "      <th>WLPR:P3</th>\n",
       "      <th>WOPR:P1</th>\n",
       "      <th>WOPR:P2</th>\n",
       "      <th>WOPR:P3</th>\n",
       "      <th>WWCT:P1</th>\n",
       "      <th>WWCT:P2</th>\n",
       "      <th>WWCT:P3</th>\n",
       "      <th>WWIR:I1</th>\n",
       "      <th>WWIR:I2</th>\n",
       "      <th>WWIR:I3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.228361</td>\n",
       "      <td>103.862024</td>\n",
       "      <td>150.795612</td>\n",
       "      <td>128.965107</td>\n",
       "      <td>124.602649</td>\n",
       "      <td>96.285832</td>\n",
       "      <td>79.988792</td>\n",
       "      <td>227.142179</td>\n",
       "      <td>101.208225</td>\n",
       "      <td>55.594877</td>\n",
       "      <td>56.711317</td>\n",
       "      <td>47.323158</td>\n",
       "      <td>0.773411</td>\n",
       "      <td>0.504997</td>\n",
       "      <td>0.347256</td>\n",
       "      <td>75.665744</td>\n",
       "      <td>227.848578</td>\n",
       "      <td>151.074577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.141899</td>\n",
       "      <td>87.267939</td>\n",
       "      <td>62.441051</td>\n",
       "      <td>80.442046</td>\n",
       "      <td>13.517003</td>\n",
       "      <td>50.088312</td>\n",
       "      <td>58.305613</td>\n",
       "      <td>61.972006</td>\n",
       "      <td>74.276796</td>\n",
       "      <td>69.840308</td>\n",
       "      <td>44.456615</td>\n",
       "      <td>45.476463</td>\n",
       "      <td>0.185950</td>\n",
       "      <td>0.315451</td>\n",
       "      <td>0.312628</td>\n",
       "      <td>64.329173</td>\n",
       "      <td>107.056419</td>\n",
       "      <td>96.376592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.993408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.349076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>172.377357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>122.052898</td>\n",
       "      <td>121.959011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>201.705578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.476236</td>\n",
       "      <td>41.662343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.751765</td>\n",
       "      <td>0.194096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>226.098743</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.204654</td>\n",
       "      <td>175.539154</td>\n",
       "      <td>175.061859</td>\n",
       "      <td>177.923561</td>\n",
       "      <td>122.062382</td>\n",
       "      <td>122.045483</td>\n",
       "      <td>121.996872</td>\n",
       "      <td>216.715744</td>\n",
       "      <td>144.311951</td>\n",
       "      <td>32.408030</td>\n",
       "      <td>51.941870</td>\n",
       "      <td>44.794483</td>\n",
       "      <td>0.844356</td>\n",
       "      <td>0.658885</td>\n",
       "      <td>0.379285</td>\n",
       "      <td>113.352001</td>\n",
       "      <td>252.142494</td>\n",
       "      <td>202.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.062971</td>\n",
       "      <td>176.582932</td>\n",
       "      <td>175.497005</td>\n",
       "      <td>178.328274</td>\n",
       "      <td>122.081005</td>\n",
       "      <td>122.080320</td>\n",
       "      <td>122.040092</td>\n",
       "      <td>232.643822</td>\n",
       "      <td>157.390774</td>\n",
       "      <td>47.587440</td>\n",
       "      <td>71.024748</td>\n",
       "      <td>69.551689</td>\n",
       "      <td>0.885562</td>\n",
       "      <td>0.763970</td>\n",
       "      <td>0.660943</td>\n",
       "      <td>132.983997</td>\n",
       "      <td>261.717499</td>\n",
       "      <td>218.379250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.915811</td>\n",
       "      <td>177.503860</td>\n",
       "      <td>190.575027</td>\n",
       "      <td>184.240341</td>\n",
       "      <td>240.400772</td>\n",
       "      <td>122.106262</td>\n",
       "      <td>122.072250</td>\n",
       "      <td>412.830994</td>\n",
       "      <td>184.237762</td>\n",
       "      <td>389.604492</td>\n",
       "      <td>220.862793</td>\n",
       "      <td>183.324097</td>\n",
       "      <td>0.917124</td>\n",
       "      <td>0.821260</td>\n",
       "      <td>0.788459</td>\n",
       "      <td>147.095001</td>\n",
       "      <td>449.602997</td>\n",
       "      <td>246.942993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           YEARS     WBHP:I1     WBHP:I2     WBHP:I3     WBHP:P1     WBHP:P2  \\\n",
       "count  90.000000   90.000000   90.000000   90.000000   90.000000   90.000000   \n",
       "mean    3.228361  103.862024  150.795612  128.965107  124.602649   96.285832   \n",
       "std     2.141899   87.267939   62.441051   80.442046   13.517003   50.088312   \n",
       "min     0.000000    0.000000    0.000000    0.000000  121.993408    0.000000   \n",
       "25%     1.349076    0.000000  172.377357    0.000000  122.052898  121.959011   \n",
       "50%     3.204654  175.539154  175.061859  177.923561  122.062382  122.045483   \n",
       "75%     5.062971  176.582932  175.497005  178.328274  122.081005  122.080320   \n",
       "max     6.915811  177.503860  190.575027  184.240341  240.400772  122.106262   \n",
       "\n",
       "          WBHP:P3     WLPR:P1     WLPR:P3     WOPR:P1     WOPR:P2     WOPR:P3  \\\n",
       "count   90.000000   90.000000   90.000000   90.000000   90.000000   90.000000   \n",
       "mean    79.988792  227.142179  101.208225   55.594877   56.711317   47.323158   \n",
       "std     58.305613   61.972006   74.276796   69.840308   44.456615   45.476463   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000  201.705578    0.000000   25.476236   41.662343    0.000000   \n",
       "50%    121.996872  216.715744  144.311951   32.408030   51.941870   44.794483   \n",
       "75%    122.040092  232.643822  157.390774   47.587440   71.024748   69.551689   \n",
       "max    122.072250  412.830994  184.237762  389.604492  220.862793  183.324097   \n",
       "\n",
       "         WWCT:P1    WWCT:P2    WWCT:P3     WWIR:I1     WWIR:I2     WWIR:I3  \n",
       "count  90.000000  90.000000  90.000000   90.000000   90.000000   90.000000  \n",
       "mean    0.773411   0.504997   0.347256   75.665744  227.848578  151.074577  \n",
       "std     0.185950   0.315451   0.312628   64.329173  107.056419   96.376592  \n",
       "min     0.000000   0.000000   0.000000    0.000000    0.000000    0.000000  \n",
       "25%     0.751765   0.194096   0.000000    0.000000  226.098743    0.000000  \n",
       "50%     0.844356   0.658885   0.379285  113.352001  252.142494  202.041000  \n",
       "75%     0.885562   0.763970   0.660943  132.983997  261.717499  218.379250  \n",
       "max     0.917124   0.821260   0.788459  147.095001  449.602997  246.942993  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I1':        WBHP:I1\n",
       " 0     0.000000\n",
       " 1     0.000000\n",
       " 2     0.000000\n",
       " 3     0.000000\n",
       " 4     0.000000\n",
       " ..         ...\n",
       " 85  177.334808\n",
       " 86  177.375671\n",
       " 87  177.418213\n",
       " 88  177.459366\n",
       " 89  177.503860\n",
       " \n",
       " [90 rows x 1 columns],\n",
       " 'I2':        WBHP:I2\n",
       " 0     0.000000\n",
       " 1     0.000000\n",
       " 2     0.000000\n",
       " 3     0.000000\n",
       " 4     0.000000\n",
       " ..         ...\n",
       " 85  175.539398\n",
       " 86  175.574432\n",
       " 87  175.608505\n",
       " 88  175.645966\n",
       " 89  175.682190\n",
       " \n",
       " [90 rows x 1 columns],\n",
       " 'I3':        WBHP:I3\n",
       " 0     0.000000\n",
       " 1     0.000000\n",
       " 2     0.000000\n",
       " 3     0.000000\n",
       " 4     0.000000\n",
       " ..         ...\n",
       " 85  178.674713\n",
       " 86  178.713562\n",
       " 87  178.751907\n",
       " 88  178.792328\n",
       " 89  178.832352\n",
       " \n",
       " [90 rows x 1 columns],\n",
       " 'P1':        WBHP:P1\n",
       " 0   240.400772\n",
       " 1   159.092148\n",
       " 2   151.187546\n",
       " 3   141.998734\n",
       " 4   134.067154\n",
       " ..         ...\n",
       " 85  122.093475\n",
       " 86  122.094536\n",
       " 87  122.096794\n",
       " 88  122.098022\n",
       " 89  122.099136\n",
       " \n",
       " [90 rows x 1 columns],\n",
       " 'P2':        WBHP:P2\n",
       " 0     0.000000\n",
       " 1     0.000000\n",
       " 2     0.000000\n",
       " 3     0.000000\n",
       " 4     0.000000\n",
       " ..         ...\n",
       " 85  122.103500\n",
       " 86  122.104240\n",
       " 87  122.104805\n",
       " 88  122.105774\n",
       " 89  122.106262\n",
       " \n",
       " [90 rows x 1 columns],\n",
       " 'P3':        WBHP:P3\n",
       " 0     0.000000\n",
       " 1     0.000000\n",
       " 2     0.000000\n",
       " 3     0.000000\n",
       " 4     0.000000\n",
       " ..         ...\n",
       " 85  122.068375\n",
       " 86  122.067650\n",
       " 87  122.069931\n",
       " 88  122.071281\n",
       " 89  122.072250\n",
       " \n",
       " [90 rows x 1 columns]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = {}\n",
    "t = {}\n",
    "for el in ['I1', 'I2', 'I3', 'P1', 'P2', 'P3']:\n",
    "    f[el] = df.filter(like=el)\n",
    "    t[el] = df.filter(like=f'WBHP:{el}')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 41.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=6, num_edges=36,\n",
       "      ndata_schemes={'emb': Scheme(shape=(60,), dtype=torch.float64)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "N_WELLS = len(f.keys())\n",
    "\n",
    "edges = [(source_node, target_node) for source_node in range(N_WELLS)\n",
    "                                    for target_node in range(N_WELLS)]\n",
    "\n",
    "N_LAGS = 25\n",
    "N_DATES = len(df.YEARS)\n",
    "\n",
    "graphs = []\n",
    "targets = []\n",
    "data = []\n",
    "\n",
    "for iteration in tqdm(f.keys()):\n",
    "    for time in range(N_LAGS, N_DATES - N_LAGS):\n",
    "        g = dgl.graph(edges, num_nodes = N_WELLS)\n",
    "        try:\n",
    "            formatted = torch.tensor(np.array(f[iteration]))\n",
    "            formatted[formatted == ''] = torch.tensor([(0,0)])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            formatted = formatted.reshape(N_WELLS, 30)\n",
    "        except:\n",
    "            formatted = formatted.reshape(N_WELLS, 60)\n",
    "        \n",
    "\n",
    "        g.ndata['emb'] = formatted\n",
    "        \n",
    "        data.append((g, t[iteration]))\n",
    "        targets.append(t[iteration]) \n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g.edges()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, train_size=0.7, random_state=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collate_fn_(batch):\n",
    "    graphs, labels = zip(*batch)\n",
    "    batched_graphs = dgl.batch(graphs)\n",
    "    batched_labels = np.concatenate(labels, axis=0)\n",
    "    return batched_graphs, batched_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_fn_)\n",
    "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE,collate_fn=collate_fn_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embs_dim = train_data[0][0].ndata['emb'].shape[1]\n",
    "hidden_dim = 16\n",
    "num_heads = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dgl import nn as dgl_nn\n",
    "from torch import nn\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.layer1 = dgl_nn.GATv2Conv(embs_dim, hidden_dim, num_heads)\n",
    "\n",
    "        self.do1 = nn.Dropout(0.2)\n",
    "        self.do2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_dim * num_heads, out_features=20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=20, out_features=1),\n",
    "        )\n",
    "                \n",
    "    def forward(self, graph):\n",
    "        x_emb = graph.ndata['emb']\n",
    "\n",
    "        h = x_emb\n",
    "        \n",
    "        h = self.layer1(graph, h)\n",
    "        h = self.do1(h)\n",
    "        \n",
    "        h = torch.reshape(h, (h.shape[0], num_heads * hidden_dim))  \n",
    "        \n",
    "        graph.ndata['h'] = h\n",
    "\n",
    "        h = self.linear(h)\n",
    "        return h  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def train_loop(model, optimizer, criterion, train_loader) -> float:\n",
    "    \"\"\"\n",
    "    Return RMSE\n",
    "    \"\"\"\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for graphs, labels in train_loader:\n",
    "        graphs = graphs.to(device)\n",
    "        labels = torch.from_numpy(labels).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(graphs).squeeze()\n",
    "        loss = criterion(predictions.to(device), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * BATCH_SIZE\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return np.sqrt(train_loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_loop(model, criterion, val_loader) -> float:\n",
    "    \"\"\"\n",
    "    Return RMSE\n",
    "    \"\"\"\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for graphs, labels in val_loader:\n",
    "        graphs = graphs.to(device)\n",
    "        labels = torch.from_numpy(labels).to(torch.float32).to(device)\n",
    "        predictions = model(graphs).squeeze()\n",
    "        loss = criterion(predictions.to(device), labels)\n",
    "        val_loss += loss.item() * BATCH_SIZE\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    return np.sqrt(val_loss)\n",
    "\n",
    "\n",
    "def plot(epochs: list[int], train_losses: list[float], val_losses: list[float], baseline_rmse: float, plot_baseline: bool):\n",
    "    if plot_baseline:\n",
    "        plt.axhline(baseline_rmse, linestyle='dashed', label='baseline')\n",
    "    plt.plot(epochs, train_losses, label='train')\n",
    "    plt.plot(epochs, val_losses, label='test')\n",
    "    plt.title('RMSE')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_gnn(model, optimizer, criterion, scheduler, train_loader, val_loader, n_epochs: int, baseline_rmse: float, plot_baseline: bool):\n",
    "    train_losses, val_losses = [], []\n",
    "    lrs = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Train\n",
    "        start_time = timer()\n",
    "        train_loss = train_loop(model, optimizer, criterion, train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_time = timer() - start_time\n",
    "\n",
    "        # Validation\n",
    "        start_time = timer()\n",
    "        val_loss = val_loop(model, criterion, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_time = timer() - start_time\n",
    "\n",
    "        # Log lr\n",
    "        lr = next(iter(optimizer.param_groups))['lr']\n",
    "        lrs.append(lr)\n",
    "\n",
    "        # Scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Log record\n",
    "        record = {\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'lr': lr,\n",
    "            'train_time': train_time,\n",
    "            'val_time': val_time,\n",
    "        }\n",
    "        wandb.log(record)\n",
    "\n",
    "        # Plot\n",
    "        if epoch % 2 == 0:\n",
    "            clear_output()\n",
    "            print(f'Epoch={epoch}')\n",
    "            print(f'val_RMSE={val_losses[-1]:.2f}; train_RMSE={train_losses[-1]:.2f}')\n",
    "            print(f'lr={lr}')\n",
    "            print(f'Train time: {train_time:.1f}s; val time: {val_time:.1f}s')\n",
    "            epochs = list(range(1, epoch + 1))\n",
    "            plot(epochs=epochs, train_losses=train_losses, val_losses=val_losses, baseline_rmse=baseline_rmse, plot_baseline=plot_baseline)\n",
    "            start_plot_epoch = 10\n",
    "            if epoch >= start_plot_epoch:\n",
    "                plot(epochs=epochs[start_plot_epoch:], train_losses=train_losses[start_plot_epoch:], val_losses=val_losses[start_plot_epoch:], baseline_rmse=baseline_rmse, plot_baseline=plot_baseline)\n",
    "            plt.plot(epochs, lrs)\n",
    "            plt.title('LR')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('LR')\n",
    "            plt.show()\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# assert device.type == 'cuda'\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000\n",
    "PATIENCE = 50\n",
    "FACTOR = 0.3\n",
    "\n",
    "KERNEL_SIZE = 10\n",
    "\n",
    "model = GAT(hidden_dim, num_heads).to(device)\n",
    "\n",
    "LR = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=PATIENCE, factor=FACTOR)\n",
    "criterion = nn.MSELoss()\n",
    "torch.manual_seed(0)\n",
    "\n",
    "experiment_name = f'GAT'\n",
    "notes = f'''\n",
    "Optimizer: Adam({LR=})\n",
    "Scheduler: ReduceLROnPlateau({PATIENCE=}, {FACTOR=})\n",
    "{hidden_dim=};\n",
    "'''\n",
    "\n",
    "wandb.init(\n",
    "    project='Oil neural network',\n",
    "    name=experiment_name,\n",
    "    notes=notes\n",
    ")\n",
    "BASELINE_RMSE = 0\n",
    "try:\n",
    "    train_gnn(model, optimizer, criterion, scheduler, train_loader=train_loader, val_loader=val_loader, n_epochs=N_EPOCHS, baseline_rmse=BASELINE_RMSE, plot_baseline=True)\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')\n",
    "    wandb.finish()\n",
    "\n",
    "print('Success')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def train_loop(model, optimizer, criterion, train_loader) -> float:\n",
    "    \"\"\"\n",
    "    Return RMSE\n",
    "    \"\"\"\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for graphs, labels in train_loader:\n",
    "        graphs = graphs.to(device)\n",
    "        labels = torch.from_numpy(labels).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(graphs).squeeze()\n",
    "        loss = criterion(predictions.to(device), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * BATCH_SIZE\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return np.sqrt(train_loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_loop(model, criterion, val_loader) -> float:\n",
    "    \"\"\"\n",
    "    Return RMSE\n",
    "    \"\"\"\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for graphs, labels in val_loader:\n",
    "        graphs = graphs.to(device)\n",
    "        labels = torch.from_numpy(labels).to(torch.float32).to(device)\n",
    "        predictions = model(graphs).squeeze()\n",
    "        loss = criterion(predictions.to(device), labels)\n",
    "        val_loss += loss.item() * BATCH_SIZE\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    return np.sqrt(val_loss)\n",
    "\n",
    "\n",
    "def plot(epochs: list[int], train_losses: list[float], val_losses: list[float], baseline_rmse: float, plot_baseline: bool):\n",
    "    if plot_baseline:\n",
    "        plt.axhline(baseline_rmse, linestyle='dashed', label='baseline')\n",
    "    plt.plot(epochs, train_losses, label='train')\n",
    "    plt.plot(epochs, val_losses, label='test')\n",
    "    plt.title('RMSE')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_gnn(model, optimizer, criterion, scheduler, train_loader, val_loader, n_epochs: int, baseline_rmse: float, plot_baseline: bool):\n",
    "    train_losses, val_losses = [], []\n",
    "    lrs = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Train\n",
    "        start_time = timer()\n",
    "        train_loss = train_loop(model, optimizer, criterion, train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_time = timer() - start_time\n",
    "\n",
    "        # Validation\n",
    "        start_time = timer()\n",
    "        val_loss = val_loop(model, criterion, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_time = timer() - start_time\n",
    "\n",
    "        # Log lr\n",
    "        lr = next(iter(optimizer.param_groups))['lr']\n",
    "        lrs.append(lr)\n",
    "\n",
    "        # Scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Log record\n",
    "        record = {\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'lr': lr,\n",
    "            'train_time': train_time,\n",
    "            'val_time': val_time,\n",
    "        }\n",
    "        wandb.log(record)\n",
    "\n",
    "        # Plot\n",
    "        if epoch % 2 == 0:\n",
    "            clear_output()\n",
    "            print(f'Epoch={epoch}')\n",
    "            print(f'val_RMSE={val_losses[-1]:.2f}; train_RMSE={train_losses[-1]:.2f}')\n",
    "            print(f'lr={lr}')\n",
    "            print(f'Train time: {train_time:.1f}s; val time: {val_time:.1f}s')\n",
    "            epochs = list(range(1, epoch + 1))\n",
    "            plot(epochs=epochs, train_losses=train_losses, val_losses=val_losses, baseline_rmse=baseline_rmse, plot_baseline=plot_baseline)\n",
    "            start_plot_epoch = 10\n",
    "            if epoch >= start_plot_epoch:\n",
    "                plot(epochs=epochs[start_plot_epoch:], train_losses=train_losses[start_plot_epoch:], val_losses=val_losses[start_plot_epoch:], baseline_rmse=baseline_rmse, plot_baseline=plot_baseline)\n",
    "            plt.plot(epochs, lrs)\n",
    "            plt.title('LR')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('LR')\n",
    "            plt.show()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# assert device.type == 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:l29s4lwd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT</strong> at: <a href='https://wandb.ai/andrros/Oil%20neural%20network/runs/l29s4lwd' target=\"_blank\">https://wandb.ai/andrros/Oil%20neural%20network/runs/l29s4lwd</a><br/> View project at: <a href='https://wandb.ai/andrros/Oil%20neural%20network' target=\"_blank\">https://wandb.ai/andrros/Oil%20neural%20network</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_233753-l29s4lwd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:l29s4lwd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a659f6075be74c2cbb11e17fd9bfa45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011258431022224614, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/andrrosya/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/gat_solution/wandb/run-20240408_233849-n1j4g124</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrros/Oil%20neural%20network/runs/n1j4g124' target=\"_blank\">GAT</a></strong> to <a href='https://wandb.ai/andrros/Oil%20neural%20network' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrros/Oil%20neural%20network' target=\"_blank\">https://wandb.ai/andrros/Oil%20neural%20network</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrros/Oil%20neural%20network/runs/n1j4g124' target=\"_blank\">https://wandb.ai/andrros/Oil%20neural%20network/runs/n1j4g124</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DGLError",
     "evalue": "Expect all graphs to have the same schema on nodes[\"_N\"].data, but graph 1 got\n\t{'emb': Scheme(shape=(30,), dtype=torch.float64)}\nwhich is different from\n\t{'emb': Scheme(shape=(60,), dtype=torch.float64)}.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mDGLError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[94], line 29\u001B[0m\n\u001B[1;32m     27\u001B[0m BASELINE_RMSE \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 29\u001B[0m     \u001B[43mtrain_gnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mN_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbaseline_rmse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBASELINE_RMSE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplot_baseline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKeyboardInterrupt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[90], line 62\u001B[0m, in \u001B[0;36mtrain_gnn\u001B[0;34m(model, optimizer, criterion, scheduler, train_loader, val_loader, n_epochs, baseline_rmse, plot_baseline)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[1;32m     61\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m---> 62\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[1;32m     64\u001B[0m     train_time \u001B[38;5;241m=\u001B[39m timer() \u001B[38;5;241m-\u001B[39m start_time\n",
      "Cell \u001B[0;32mIn[90], line 13\u001B[0m, in \u001B[0;36mtrain_loop\u001B[0;34m(model, optimizer, criterion, train_loader)\u001B[0m\n\u001B[1;32m     11\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m     12\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m graphs, labels \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m     14\u001B[0m     graphs \u001B[38;5;241m=\u001B[39m graphs\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     15\u001B[0m     labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(labels)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[84], line 3\u001B[0m, in \u001B[0;36mcollate_fn_\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcollate_fn_\u001B[39m(batch):\n\u001B[1;32m      2\u001B[0m     graphs, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch)\n\u001B[0;32m----> 3\u001B[0m     batched_graphs \u001B[38;5;241m=\u001B[39m \u001B[43mdgl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraphs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     batched_labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(labels, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m batched_graphs, batched_labels\n",
      "File \u001B[0;32m~/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages/dgl/batch.py:201\u001B[0m, in \u001B[0;36mbatch\u001B[0;34m(graphs, ndata, edata)\u001B[0m\n\u001B[1;32m    194\u001B[0m         frames \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    195\u001B[0m             g\u001B[38;5;241m.\u001B[39m_node_frames[ntype_id]\n\u001B[1;32m    196\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m graphs\n\u001B[1;32m    197\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m g\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39mnum_nodes(ntype_id) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m all_empty\n\u001B[1;32m    198\u001B[0m         ]\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO: do we require graphs with no nodes/edges to have the same schema?  Currently\u001B[39;00m\n\u001B[1;32m    200\u001B[0m         \u001B[38;5;66;03m# we allow empty graphs to have no features during batching.\u001B[39;00m\n\u001B[0;32m--> 201\u001B[0m         ret_feat \u001B[38;5;241m=\u001B[39m \u001B[43m_batch_feat_dicts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m            \u001B[49m\u001B[43mframes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnodes[\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m].data\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mntype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    204\u001B[0m         retg\u001B[38;5;241m.\u001B[39mnodes[ntype]\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mupdate(ret_feat)\n\u001B[1;32m    206\u001B[0m \u001B[38;5;66;03m# Batch edge feature\u001B[39;00m\n",
      "File \u001B[0;32m~/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages/dgl/batch.py:247\u001B[0m, in \u001B[0;36m_batch_feat_dicts\u001B[0;34m(frames, keys, feat_dict_name)\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;66;03m# sanity checks\u001B[39;00m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_all(keys):\n\u001B[0;32m--> 247\u001B[0m     \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_all_same_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mschemas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeat_dict_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m     keys \u001B[38;5;241m=\u001B[39m schemas[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Downloads/Forecasting-the-operation-of-wells-using-graph-neural-networks-main/venv/lib/python3.9/site-packages/dgl/utils/checks.py:207\u001B[0m, in \u001B[0;36mcheck_all_same_schema\u001B[0;34m(schemas, name)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, schema \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(schemas):\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;241m!=\u001B[39m schemas[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m--> 207\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m DGLError(\n\u001B[1;32m    208\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpect all graphs to have the same schema on \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut graph \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m got\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mwhich is different from\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    210\u001B[0m                 name, i, schema, schemas[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    211\u001B[0m             )\n\u001B[1;32m    212\u001B[0m         )\n",
      "\u001B[0;31mDGLError\u001B[0m: Expect all graphs to have the same schema on nodes[\"_N\"].data, but graph 1 got\n\t{'emb': Scheme(shape=(30,), dtype=torch.float64)}\nwhich is different from\n\t{'emb': Scheme(shape=(60,), dtype=torch.float64)}."
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1000\n",
    "PATIENCE = 50\n",
    "FACTOR = 0.3\n",
    "\n",
    "KERNEL_SIZE = 10\n",
    "\n",
    "model = GAT(hidden_dim, num_heads).to(device)\n",
    "\n",
    "LR = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=PATIENCE, factor=FACTOR)\n",
    "criterion = nn.MSELoss()\n",
    "torch.manual_seed(0)\n",
    "\n",
    "experiment_name = f'GAT'\n",
    "notes = f'''\n",
    "Optimizer: Adam({LR=})\n",
    "Scheduler: ReduceLROnPlateau({PATIENCE=}, {FACTOR=})\n",
    "{hidden_dim=};\n",
    "'''\n",
    "\n",
    "wandb.init(\n",
    "    project='Oil neural network',\n",
    "    name=experiment_name,\n",
    "    notes=notes\n",
    ")\n",
    "BASELINE_RMSE = 0\n",
    "try:\n",
    "    train_gnn(model, optimizer, criterion, scheduler, train_loader=train_loader, val_loader=val_loader, n_epochs=N_EPOCHS, baseline_rmse=BASELINE_RMSE, plot_baseline=True)\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')\n",
    "    wandb.finish()\n",
    "\n",
    "print('Success')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}